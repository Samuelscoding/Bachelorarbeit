{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports und Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmium as osm\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from collections import Counter, defaultdict\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import xml.etree.ElementTree as ET\n",
    "from matplotlib.lines import Line2D\n",
    "import random\n",
    "\n",
    "# Setze den Pfad zum Dataset\n",
    "dataset_path = os.path.join(\"Datasets\", \"unterfranken-latest.osm\")\n",
    "cities = [\"Ringheim\", \"Mömlingen\", \"Glattbach\", \"Großostheim\", \"Marktheidenfeld\", \"Aschaffenburg\", \"Würzburg\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten in XML-Datei speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_xml(df, city_name, addresstype):\n",
    "    root = ET.Element(\"osm\", version=\"0.6\", generator=\"Bachelorproject\")\n",
    "    \n",
    "    # Knoten hinzufügen\n",
    "    for _, row in df[df['type'] == 'node'].iterrows():\n",
    "        node = ET.SubElement(root, \"node\", id=str(row['id']),\n",
    "                             visible=str(row['visible']),\n",
    "                             lat=str(row['latitude']),\n",
    "                             lon=str(row['longitude']))\n",
    "        if row['tagkey']:\n",
    "            ET.SubElement(node, \"tag\", k=row['tagkey'], v=row['tagvalue'])\n",
    "    \n",
    "    # Wege hinzufügen\n",
    "    for _, row in df[df['type'] == 'way'].iterrows():\n",
    "        way_attrs = {\n",
    "            \"id\": str(row['id']),\n",
    "            \"visible\": str(row['visible'])\n",
    "        }\n",
    "        # Wenn eine Länge vorhanden ist, als Attribut hinzufügen\n",
    "        if 'length' in row and pd.notna(row['length']):\n",
    "            way_attrs['length'] = str(row['length'])\n",
    "        \n",
    "        way = ET.SubElement(root, \"way\", **way_attrs)\n",
    "        \n",
    "        for node_id in row['node_ids']:\n",
    "            ET.SubElement(way, \"nd\", ref=str(node_id))\n",
    "        \n",
    "        if row['tagkey']:\n",
    "            ET.SubElement(way, \"tag\", k=row['tagkey'], v=row['tagvalue'])\n",
    "\n",
    "    # XML-Baum erstellen und speichern\n",
    "    city_folder = os.path.join(\"City_data\", addresstype)\n",
    "    os.makedirs(city_folder, exist_ok=True)\n",
    "    \n",
    "    tree = ET.ElementTree(root)\n",
    "    file_path = os.path.join(city_folder, f\"{city_name}.osm\")\n",
    "    tree.write(file_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "    print(f\"Datei erfolgreich gespeichert: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten aus der XML-Datei lesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_xml(city_name, addresstype):\n",
    "    file_path = os.path.join(\"City_data\", addresstype,f\"{city_name}.osm\")\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    osm_data = []\n",
    "\n",
    "    # Knoten extrahieren\n",
    "    for node in root.findall(\"node\"):\n",
    "        node_id = int(node.get(\"id\"))\n",
    "        lon = float(node.get(\"lon\"))\n",
    "        lat = float(node.get(\"lat\"))\n",
    "        osm_data.append([\"node\", node_id, True, 0, None, None, lon, lat, None])\n",
    "\n",
    "    # Wege extrahieren\n",
    "    for way in root.findall(\"way\"):\n",
    "        way_id = int(way.get(\"id\"))\n",
    "        node_ids = [int(nd.get(\"ref\")) for nd in way.findall(\"nd\")]\n",
    "        for tag in way.findall(\"tag\"):\n",
    "            tagkey = tag.get(\"k\")\n",
    "            tagvalue = tag.get(\"v\")\n",
    "            osm_data.append([\"way\", way_id, True, len(way.findall(\"tag\")), tagkey, tagvalue, None, None, node_ids])\n",
    "\n",
    "    # DataFrame erstellen\n",
    "    columns = ['type', 'id', 'visible', 'ntags', 'tagkey', 'tagvalue', 'longitude', 'latitude', 'node_ids']\n",
    "    return pd.DataFrame(osm_data, columns=columns)\n",
    "\n",
    "def load_from_xml_with_length(city_name, addresstype):\n",
    "    file_path = os.path.join(\"City_data\", addresstype,f\"{city_name}.osm\")\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    osm_data = []\n",
    "\n",
    "    # Knoten extrahieren\n",
    "    for node in root.findall(\"node\"):\n",
    "        node_id = int(node.get(\"id\"))\n",
    "        lon = float(node.get(\"lon\"))\n",
    "        lat = float(node.get(\"lat\"))\n",
    "        osm_data.append([\"node\", node_id, True, 0, None, None, lon, lat, None])\n",
    "\n",
    "    # Wege extrahieren\n",
    "    for way in root.findall(\"way\"):\n",
    "        way_id = int(way.get(\"id\"))\n",
    "        node_ids = [int(nd.get(\"ref\")) for nd in way.findall(\"nd\")]\n",
    "        length = way.get(\"length\")  \n",
    "        length = float(length) if length else None  \n",
    "        for tag in way.findall(\"tag\"):\n",
    "            tagkey = tag.get(\"k\")\n",
    "            tagvalue = tag.get(\"v\")\n",
    "            osm_data.append([\"way\", way_id, True, len(way.findall(\"tag\")), tagkey, tagvalue, None, None, node_ids, length])\n",
    "\n",
    "    # DataFrame erstellen\n",
    "    columns = ['type', 'id', 'visible', 'ntags', 'tagkey', 'tagvalue', 'longitude', 'latitude', 'node_ids', 'length']\n",
    "    return pd.DataFrame(osm_data, columns=columns)\n",
    "\n",
    "def load_from_all_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    osm_data = []\n",
    "\n",
    "    # Knoten extrahieren\n",
    "    for node in root.findall(\"node\"):\n",
    "        node_id = int(node.get(\"id\"))\n",
    "        lon = float(node.get(\"lon\"))\n",
    "        lat = float(node.get(\"lat\"))\n",
    "        osm_data.append([\"node\", node_id, True, 0, None, None, lon, lat, None])\n",
    "\n",
    "    # Wege extrahieren\n",
    "    for way in root.findall(\"way\"):\n",
    "        way_id = int(way.get(\"id\"))\n",
    "        node_ids = [int(nd.get(\"ref\")) for nd in way.findall(\"nd\")]\n",
    "        length = way.get(\"length\")  \n",
    "        length = float(length) if length else None  \n",
    "        for tag in way.findall(\"tag\"):\n",
    "            tagkey = tag.get(\"k\")\n",
    "            tagvalue = tag.get(\"v\")\n",
    "            osm_data.append([\"way\", way_id, True, len(way.findall(\"tag\")), tagkey, tagvalue, None, None, node_ids, length])\n",
    "\n",
    "    # DataFrame erstellen\n",
    "    columns = ['type', 'id', 'visible', 'ntags', 'tagkey', 'tagvalue', 'longitude', 'latitude', 'node_ids', 'length']\n",
    "    return pd.DataFrame(osm_data, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haversine-Formel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # Dezimalzahlen in Radianten umwandeln\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # Haversine Formel\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius der Erde in Kilometer\n",
    "    return c * r * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Straßenlängen berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Straßenlänge berechnen\n",
    "def calculate_full_road_length(way_row, df_nodes):\n",
    "    node_ids = way_row['node_ids']\n",
    "    total_length = 0\n",
    "\n",
    "    for i in range(len(node_ids) - 1):\n",
    "        first_node = df_nodes[df_nodes['id'] == node_ids[i]]\n",
    "        second_node = df_nodes[df_nodes['id'] == node_ids[i + 1]]\n",
    "\n",
    "        # Überprüfung, ob beide Knoten existieren\n",
    "        if first_node.empty or second_node.empty:\n",
    "            return None\n",
    "\n",
    "        total_length += haversine(\n",
    "            first_node['longitude'].values[0], first_node['latitude'].values[0],\n",
    "            second_node['longitude'].values[0], second_node['latitude'].values[0]\n",
    "        )\n",
    "\n",
    "    return total_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kreuzungsknoten erhalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_intersection_nodes(df):\n",
    "    all_node_ids = []\n",
    "    for node_ids_list in df[df['type'] == 'way']['node_ids']:\n",
    "        if node_ids_list is not None:\n",
    "            all_node_ids.extend(node_ids_list)\n",
    "\n",
    "    node_reference_count = Counter(all_node_ids)\n",
    "    nodes_referenced_multiple = {node_id for node_id, count in node_reference_count.items() if count > 1}\n",
    "\n",
    "    df_nodes_filtered_multiple = df[(df['type'] == 'node') & (df['id'].isin(nodes_referenced_multiple))]\n",
    "\n",
    "    filtered_data_multiple = pd.concat([df[df['type'] == 'way'], df_nodes_filtered_multiple], ignore_index=True)\n",
    "\n",
    "    return filtered_data_multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knoten vom Grad 2 entfernen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funktionen für die Bearbeitung der Grad 2 Knoten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_found = {}\n",
    "\n",
    "# Entfernen von Knoten mit Grad 2 aus den node-ids\n",
    "def remove_degree_2_nodes(node_ids, degree_2_node_ids):\n",
    "    if node_ids is not None:\n",
    "        return [node_id for node_id in node_ids if node_id not in degree_2_node_ids]\n",
    "    return node_ids\n",
    "\n",
    "# 2 Grad-3+-Nachbarn finden\n",
    "def find_neighbors_with_degree_3(G, node_id, visited=None, found_neighbors=None):    \n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if found_neighbors is None:\n",
    "        found_neighbors = []\n",
    "\n",
    "    visited.add(node_id)\n",
    "    neighbors = list(G.neighbors(node_id))\n",
    "\n",
    "    for neighbor in neighbors:\n",
    "        if neighbor not in visited:\n",
    "            neighbor_degree = G.degree(neighbor)\n",
    "\n",
    "            # Füge Nachbarn hinzu, wenn er Grad 3 oder höher hat\n",
    "            if neighbor_degree >= 3:\n",
    "                found_neighbors.append(neighbor)\n",
    "\n",
    "                if node_id not in neighbors_found:\n",
    "                    neighbors_found[node_id] = []\n",
    "                neighbors_found[node_id].append(neighbor)\n",
    "\n",
    "                # Stoppe, wenn zwei passende Nachbarn gefunden wurden\n",
    "                if len(found_neighbors) == 2:\n",
    "                    return found_neighbors\n",
    "\n",
    "            # Rekursive Suche fortsetzen, wenn der Nachbar Grad 2 hat\n",
    "            elif neighbor_degree == 2:\n",
    "                result = find_neighbors_with_degree_3(G, neighbor, visited, found_neighbors)\n",
    "                if len(result) == 2:\n",
    "                    return result\n",
    "                \n",
    "    return found_neighbors\n",
    "\n",
    "# Aktualisieren der node_ids der Straßen, die Knoten mit Grad 2 enthalten\n",
    "def update_node_ids(G, row, degree_2_node_ids, ways_filter):\n",
    "    node_ids = row['node_ids']\n",
    "    if node_ids is not None:\n",
    "        updated_node_ids = list(node_ids)\n",
    "        for node_id in node_ids:\n",
    "            if node_id in degree_2_node_ids:\n",
    "                # Finde Nachbarn des Knotens mit Grad 2\n",
    "                neighbors = find_neighbors_with_degree_3(G, node_id)\n",
    "                if len(neighbors) == 2:\n",
    "                    neighbor_1, neighbor_2 = neighbors\n",
    "\n",
    "                    # Finde die zwei Straßen, die den Grad-2-Knoten referenzieren\n",
    "                    referencing_ways = ways_filter[ways_filter['node_ids'].apply(lambda x: node_id in x)]\n",
    "\n",
    "                    if len(referencing_ways) == 2:\n",
    "                        way_1, way_2 = referencing_ways.iloc[0], referencing_ways.iloc[1]\n",
    "\n",
    "                        # Kopiere die Node-IDs als Listen\n",
    "                        way_1_node_ids = list(way_1['node_ids']) if isinstance(way_1['node_ids'], list) else way_1['node_ids']\n",
    "                        way_2_node_ids = list(way_2['node_ids']) if isinstance(way_2['node_ids'], list) else way_2['node_ids']\n",
    "\n",
    "                        # Füge Nachbarn hinzu, falls sie nicht bereits vorhanden sind\n",
    "                        if neighbor_1 not in way_2_node_ids:\n",
    "                            way_2_node_ids.append(neighbor_1)\n",
    "                        if neighbor_1 not in way_1_node_ids:\n",
    "                            way_1_node_ids.append(neighbor_1)\n",
    "\n",
    "                        # Aktualisiere die DataFrame-Zeilen\n",
    "                        ways_filter.at[way_1.name, 'node_ids'] = way_1_node_ids\n",
    "                        ways_filter.at[way_2.name, 'node_ids'] = way_2_node_ids\n",
    "\n",
    "        # Stelle sicher, dass die Rückgabewerte korrekt sind\n",
    "        return updated_node_ids if updated_node_ids else None\n",
    "    return node_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funktionen für die Bearbeitung der speziellen Grad-2-Knoten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_degree_2_nodes(special_nodes, connected_ways, ways_filter, G):\n",
    "    neighbors_by_special_node = {}\n",
    "\n",
    "    def find_special_neighbors(node_id, connected_node_ids, visited=None, found_neighbors=None):\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        if found_neighbors is None:\n",
    "            found_neighbors = set()\n",
    "\n",
    "        visited.add(node_id)\n",
    "        neighbors = list(G.neighbors(node_id))\n",
    "\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor not in visited and neighbor in connected_node_ids:  \n",
    "                neighbor_degree = G.degree(neighbor)\n",
    "\n",
    "                # Füge Nachbarn hinzu, wenn er Grad 3 oder höher hat\n",
    "                if neighbor_degree >= 3:\n",
    "                    found_neighbors.add(neighbor) \n",
    "\n",
    "                    # Stoppe, wenn zwei passende Nachbarn gefunden wurden\n",
    "                    if len(found_neighbors) == 2:\n",
    "                        return list(found_neighbors)\n",
    "\n",
    "                # Rekursive Suche fortsetzen, wenn der Nachbar Grad 2 hat\n",
    "                elif neighbor_degree == 2:\n",
    "                    result = find_special_neighbors(neighbor, connected_node_ids, visited, found_neighbors)\n",
    "                    if len(result) == 2:\n",
    "                        return list(result)\n",
    "\n",
    "        return list(found_neighbors)\n",
    "\n",
    "    # Für jeden speziellen Knoten die Nachbarn durch verbundene Wege finden\n",
    "    for node_id, ways in connected_ways.items():\n",
    "        neighbors_by_special_node[node_id] = []\n",
    "        connected_node_ids = set()\n",
    "        for way_id in ways:\n",
    "            way = ways_filter[ways_filter['id'] == way_id]\n",
    "            if not way.empty:\n",
    "                connected_node_ids.update(way.iloc[0]['node_ids'])\n",
    "\n",
    "        # Suche Nachbarn des speziellen Knotens, die Grad 3+ haben und in connected_node_ids liegen\n",
    "        neighbors = find_special_neighbors(node_id, connected_node_ids)\n",
    "        if neighbors:\n",
    "            neighbors_by_special_node[node_id].extend(neighbors)\n",
    "\n",
    "    return neighbors_by_special_node\n",
    "\n",
    "# Identifizieren der speziellen Grad-2-Knoten\n",
    "def identify_special_degree_2_nodes(node_way_mapping):\n",
    "    special_nodes = {node_id: ways for node_id, ways in node_way_mapping.items() if len(ways) > 2}\n",
    "    return special_nodes\n",
    "\n",
    "# Ermitteln der Straßen, die mit speziellen Grad-2-Knoten verbunden sind\n",
    "def get_connected_ways_for_special_nodes(df_special_nodes, valid_ways, G):\n",
    "    connected_ways_by_node = {}\n",
    "\n",
    "    for _, node_row in df_special_nodes.iterrows():\n",
    "        node_id = node_row['id']\n",
    "        connected_ways_by_node[node_id] = []\n",
    "        \n",
    "        # Finde Wege, die mit diesem speziellen Knoten verbunden sind\n",
    "        for _, way_row in valid_ways.iterrows():\n",
    "            way_id = way_row['id']\n",
    "            node_ids_list = way_row['node_ids']\n",
    "            \n",
    "            if node_id in node_ids_list:\n",
    "                # Prüfen, ob die Knoten des Wegs im Graphen existieren\n",
    "                for i in range(len(node_ids_list) - 1):\n",
    "                    u, v = node_ids_list[i], node_ids_list[i + 1]\n",
    "                    if G.has_edge(u, v):\n",
    "                        connected_ways_by_node[node_id].append(way_id)\n",
    "                        break\n",
    "\n",
    "    return connected_ways_by_node\n",
    "\n",
    "# Aktualisieren der node_ids für spezielle Knoten\n",
    "def update_special_node_ids(row, special_nodes, neighbors_by_special_node, connected_ways_for_special_nodes, ways_filter):\n",
    "    node_ids = row['node_ids']\n",
    "\n",
    "    if not isinstance(node_ids, list):\n",
    "        node_ids = []\n",
    "    \n",
    "    if node_ids:\n",
    "        updated_node_ids = list(node_ids)\n",
    "        \n",
    "        for node_id in node_ids:\n",
    "            if node_id in special_nodes:\n",
    "                neighbors = neighbors_by_special_node.get(node_id, [])\n",
    "        \n",
    "                if len(neighbors) == 2:\n",
    "                    neighbor_1, neighbor_2 = neighbors\n",
    "                    referencing_ways = connected_ways_for_special_nodes.get(node_id, [])\n",
    "                    if len(referencing_ways) == 2:\n",
    "                        way_1_id, way_2_id = referencing_ways[0], referencing_ways[1]\n",
    "                        \n",
    "                        way_1 = ways_filter[ways_filter['id'] == way_1_id].iloc[0]\n",
    "                        way_2 = ways_filter[ways_filter['id'] == way_2_id].iloc[0]\n",
    "\n",
    "                        way_1_node_ids = list(way_1['node_ids']) if isinstance(way_1['node_ids'], list) else way_1['node_ids']\n",
    "                        way_2_node_ids = list(way_2['node_ids']) if isinstance(way_2['node_ids'], list) else way_2['node_ids']\n",
    "\n",
    "                        if neighbor_1 not in way_2_node_ids:\n",
    "                            way_2_node_ids.append(neighbor_1)\n",
    "                        if neighbor_1 not in way_1_node_ids:\n",
    "                            way_1_node_ids.append(neighbor_1)\n",
    "\n",
    "                        ways_filter.at[way_1.name, 'node_ids'] = way_1_node_ids\n",
    "                        ways_filter.at[way_2.name, 'node_ids'] = way_2_node_ids\n",
    "        \n",
    "        return updated_node_ids if updated_node_ids else None\n",
    "    \n",
    "    return node_ids\n",
    "\n",
    "# Entfernen der speziellen und normalen Knoten\n",
    "def remove_special_nodes_from_ids(node_ids, special_node_ids):\n",
    "    if node_ids is not None:\n",
    "        return [node_id for node_id in node_ids if node_id not in special_node_ids]\n",
    "    return node_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufruf zum Entfernen der Grad-2-Knoten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_degree_2_nodes_from_file(df, ways_filter):\n",
    "    # Extrahieren der Knoten_Ids\n",
    "    existing_node_ids = set(df[df['type'] == 'node']['id'])\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for index, row in df[df['type'] == 'node'].iterrows():  \n",
    "        G.add_node(row['id'], pos=(row['longitude'], row['latitude']))\n",
    "\n",
    "    # Wege durchgehen und Kanten im Graphen hinzufügen\n",
    "    for _, row in df[df['type'] == 'way'].iterrows():\n",
    "        node_ids = row['node_ids']\n",
    "        if node_ids is not None:\n",
    "            valid_node_ids = [node_id for node_id in node_ids if node_id in existing_node_ids]\n",
    "            for i in range(len(valid_node_ids) - 1):\n",
    "                G.add_edge(valid_node_ids[i], valid_node_ids[i + 1])\n",
    "\n",
    "    # Knoten mit Grad 2 finden\n",
    "    node_degrees = dict(G.degree())\n",
    "    nodes_with_degree_2 = [node_id for node_id, degree in node_degrees.items() if degree == 2]\n",
    "\n",
    "    # Knoten mit Grad 2 in filtered_data_multiple filtern\n",
    "    df_nodes_degree_2 = df[(df['type'] == 'node') & (df['id'].isin(nodes_with_degree_2))]\n",
    "    degree_2_node_ids = set(df_nodes_degree_2['id'])\n",
    "\n",
    "    # Spezielle Knoten mit Grad 2 finden\n",
    "    nodes_degree_2 = [node for node, degree in G.degree() if degree == 2]\n",
    "    node_way_mapping = {node_id: [] for node_id in nodes_degree_2}\n",
    "    for index, way in df[df['type'] == 'way'].iterrows():\n",
    "        node_ids_list = way['node_ids']\n",
    "        if node_ids_list is not None:\n",
    "            for node_id in nodes_degree_2:\n",
    "                if node_id in node_ids_list:\n",
    "                    node_way_mapping[node_id].append(way['id'])\n",
    "    special_nodes = identify_special_degree_2_nodes(node_way_mapping)\n",
    "\n",
    "    # Datensatz für spezielle Knoten\n",
    "    special_node_ids = set(special_nodes.keys()) \n",
    "    df_special_nodes = df[df['id'].isin(special_node_ids)]\n",
    "\n",
    "    # Straßen mit speziellen Knoten ermitteln\n",
    "    connected_ways_for_special_nodes = get_connected_ways_for_special_nodes(df_special_nodes, df[df['type'] == 'way'], G)\n",
    "\n",
    "    # Nachbarn von speziellen Knoten ermitteln\n",
    "    neighbors_by_special_node = handle_special_degree_2_nodes(df_special_nodes, connected_ways_for_special_nodes, ways_filter, G)\n",
    "\n",
    "    # Aktualisieren die node_ids der speziellen Straßen\n",
    "    df['node_ids'] = df.apply(lambda row: update_special_node_ids(row, special_nodes, neighbors_by_special_node, connected_ways_for_special_nodes, ways_filter), axis=1)\n",
    "\n",
    "    # Aktualisiere die node_ids der Straßen\n",
    "    df['node_ids'] = df.apply(lambda row: update_node_ids(G, row, degree_2_node_ids, ways_filter), axis=1)\n",
    "\n",
    "    # Entfernen der Knoten mit Grad 2 aus den node_ids der Straßen\n",
    "    df['node_ids'] = df['node_ids'].apply(lambda node_ids: remove_special_nodes_from_ids(node_ids, degree_2_node_ids.union(special_node_ids)))\n",
    "\n",
    "    # Entferne Knoten mit Grad 2 aus dem Datensatz\n",
    "    df = df[~df['id'].isin(degree_2_node_ids.union(special_node_ids))]\n",
    "\n",
    "    G.remove_nodes_from(nodes_with_degree_2)\n",
    "\n",
    "    # Entferne unverbundene Knoten aus dem Datensatz\n",
    "    connected_nodes = set(G.nodes)\n",
    "    df = df[(df['type'] != 'node') | (df['id'].isin(connected_nodes))]\n",
    "    isolated_nodes = [node for node, degree in G.degree() if degree == 0]\n",
    "    df = df[~((df['type'] == 'node') & (df['id'].isin(isolated_nodes)))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knoten zusammenfügen, die nah beieinander stehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_merging(df, max_distance=5):\n",
    "    df_nodes = df[df['type'] == 'node']\n",
    "    df_ways = df[df['type'] == 'way']\n",
    "\n",
    "    merged_nodes, node_merge_dict = merge_within_distance(df_nodes, max_distance)\n",
    "    df_merged_nodes = pd.DataFrame(merged_nodes, columns=['id', 'longitude', 'latitude'])\n",
    "\n",
    "    # Wege aktualisieren\n",
    "    df_ways['node_ids'] = df_ways['node_ids'].apply(\n",
    "        lambda x: update_node_ids_merged(x, node_merge_dict)\n",
    "    )\n",
    "    # Ehemalige Knoten entfernen\n",
    "    df_nodes_updated = remove_replaced_nodes(df_nodes, node_merge_dict)\n",
    "\n",
    "    df_nodes_final = pd.concat([df_nodes_updated, df_merged_nodes], ignore_index=True)\n",
    "\n",
    "    # Doppelte und leere Knoten-IDs entfernen\n",
    "    df_ways = df_ways[df_ways['node_ids'].apply(lambda x: len(x) > 1)]\n",
    "\n",
    "    df_city_data_updated = pd.concat([df_nodes_final, df_ways], ignore_index=True)\n",
    "\n",
    "    return df_city_data_updated\n",
    "\n",
    "    # Löschen der Knoten, die durch anderen ersetzt wurde\n",
    "def remove_replaced_nodes(df_nodes, node_merge_dict):\n",
    "    replaced_node_ids = set(node_merge_dict.keys()) - set(node_merge_dict.values())\n",
    "    return df_nodes[~df_nodes['id'].isin(replaced_node_ids)]\n",
    "\n",
    "# Aktualisiere node_ids\n",
    "def update_node_ids_merged(node_ids, node_merge_dict):\n",
    "    if node_ids is not None and isinstance(node_ids, list):\n",
    "        updated_node_ids = [node_merge_dict.get(node_id, node_id) for node_id in node_ids]\n",
    "        # Entferne doppelte Knoten, falls sie durch das Zusammenführen entstanden sind\n",
    "        unique_node_ids = list(dict.fromkeys(updated_node_ids))\n",
    "        return unique_node_ids\n",
    "    return node_ids\n",
    "\n",
    "# Zusammenfügen der Knoten, die nah beieinander stehen\n",
    "def merge_within_distance(df_nodes, max_distance):\n",
    "    merged_nodes = []\n",
    "    node_merge_dict = {}\n",
    "    seen = set()\n",
    "\n",
    "    for i, node_1 in df_nodes.iterrows():\n",
    "        for j, node_2 in df_nodes.iterrows():\n",
    "            # Vergleich gleicher Knoten verhindern\n",
    "            if node_1['id'] == node_2['id'] or node_1['id'] in seen or node_2['id'] in seen:\n",
    "                continue\n",
    "\n",
    "            lon1, lat1 = node_1['longitude'], node_1['latitude']\n",
    "            lon2, lat2 = node_2['longitude'], node_2['latitude']\n",
    "\n",
    "            distance = haversine(lon1, lat1, lon2, lat2)\n",
    "\n",
    "            if distance < max_distance:\n",
    "                merged_node = {\n",
    "                    'id': node_1['id'],\n",
    "                    'longitude': (lon1 + lon2) / 2,\n",
    "                    'latitude': (lat1 + lat2) / 2\n",
    "                }\n",
    "                merged_nodes.append(merged_node)\n",
    "\n",
    "                # Knoten als gesehen markiert\n",
    "                seen.add(node_1['id'])\n",
    "                seen.add(node_2['id'])\n",
    "\n",
    "                node_merge_dict[node_2['id']] = node_1['id']\n",
    "\n",
    "    return merged_nodes, node_merge_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolierte Knoten entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_isolated_nodes(df):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for index, row in df[df['type'] == 'node'].iterrows():  \n",
    "        G.add_node(row['id'], pos=(row['longitude'], row['latitude']))\n",
    "\n",
    "    for index, way in df[df['type'] == 'way'].iterrows():\n",
    "        node_ids_list = way['node_ids']\n",
    "        if node_ids_list is not None:\n",
    "            filtered_node_ids_list = [node_id for node_id in node_ids_list if G.has_node(node_id)]\n",
    "            if len(filtered_node_ids_list) > 1:\n",
    "                for i in range(len(filtered_node_ids_list) - 1):\n",
    "                    G.add_edge(filtered_node_ids_list[i], filtered_node_ids_list[i + 1])\n",
    "\n",
    "    connected_nodes = set(G.nodes)\n",
    "    df = df[(df['type'] != 'node') | (df['id'].isin(connected_nodes))]\n",
    "    isolated_nodes = [node for node, degree in G.degree() if degree == 0]\n",
    "    df = df[~((df['type'] == 'node') & (df['id'].isin(isolated_nodes)))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alle Daten aus dem Datensatz auslesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSMHandler(osm.SimpleHandler):\n",
    "    def __init__(self):\n",
    "        osm.SimpleHandler.__init__(self)\n",
    "        self.osm_data = []\n",
    "\n",
    "    def tag_inventory(self, elem, elem_type, lon=None, lat=None, node_ids=None):\n",
    "        if len(elem.tags) == 0:\n",
    "            # Für Knoten ohne Tags\n",
    "            self.osm_data.append([elem_type, \n",
    "                                  elem.id, \n",
    "                                  elem.visible,\n",
    "                                  0, \n",
    "                                  None, \n",
    "                                  None, \n",
    "                                  lon, \n",
    "                                  lat, \n",
    "                                  node_ids])\n",
    "        else:\n",
    "            for tag in elem.tags:\n",
    "                self.osm_data.append([elem_type, \n",
    "                                      elem.id,\n",
    "                                      elem.visible,\n",
    "                                      len(elem.tags),\n",
    "                                      tag.k, \n",
    "                                      tag.v, lon, lat, node_ids])\n",
    "\n",
    "    def node(self, n):\n",
    "        self.tag_inventory(n, \"node\", lon=n.location.lon, lat=n.location.lat)\n",
    "\n",
    "    def way(self, w):\n",
    "        node_ids = [n.ref for n in w.nodes]  \n",
    "        self.tag_inventory(w, \"way\", node_ids=node_ids)\n",
    "\n",
    "# Daten auslesen\n",
    "osmhandler = OSMHandler()\n",
    "osmhandler.apply_file(dataset_path)\n",
    "\n",
    "# DataFrame erstellen\n",
    "data_colnames = ['type', 'id', 'visible', 'ntags', 'tagkey', 'tagvalue', 'longitude', 'latitude', 'node_ids']\n",
    "df_osm = pd.DataFrame(osmhandler.osm_data, columns=data_colnames)\n",
    "df_osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datensatz nach Wegen gefiltert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ways_filter = df_osm[(df_osm['type'] == 'way') & (df_osm['tagkey'] == 'highway')]\n",
    "ways_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ways_maxspeed = df_osm[(df_osm['type'] == 'way') & (df_osm['tagkey'] == 'maxspeed')]\n",
    "ways_maxspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API-Anfrage für die Längen- und Breitengrade der Städte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API-Anfrage um Bounding Box der Stadt zu bekommen\n",
    "def get_bounding_box(city_name):\n",
    "    url = f\"https://nominatim.openstreetmap.org/search?city={city_name}&format=json\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Bachelorproject/1.0\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        for result in results:\n",
    "            if result.get(\"osm_type\") in [\"node\", \"relation\"]:\n",
    "                if result.get(\"addresstype\") in [\"village\", \"town\", \"city\"] and 'boundingbox' in result:\n",
    "                    display_name = result.get(\"display_name\", \"\")\n",
    "                    if 'Baden-Württemberg' in display_name:\n",
    "                        bbox = result['boundingbox']\n",
    "                        addresstype = result.get(\"addresstype\", \"\")\n",
    "                        return [float(coord) for coord in bbox], addresstype  \n",
    "    raise ValueError(\"Bounding Box konnte nicht abgerufen werden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datensatz nach Städten filtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cities(cities, ways_filter):\n",
    "    for city in cities:\n",
    "        try:\n",
    "            for i in range(2):\n",
    "                print(f\"Processing {city}...\")\n",
    "                bbox, addresstype = get_bounding_box(city)\n",
    "                south, north, west, east = bbox\n",
    "\n",
    "                df_filtered_nodes = df_osm[\n",
    "                    ((df_osm['latitude'] >= south) & (df_osm['latitude'] <= north)) &\n",
    "                    ((df_osm['longitude'] >= west) & (df_osm['longitude'] <= east))    \n",
    "                ]\n",
    "\n",
    "                print(\"Processing City filter...\")\n",
    "                ways = ways_filter[ways_filter['node_ids'].apply(lambda node_ids: \n",
    "                    isinstance(node_ids, list) and any(node in df_filtered_nodes['id'].values for node in node_ids))]\n",
    "            \n",
    "                road_lengths = []\n",
    "                for index, way_row in ways.iterrows():\n",
    "                    length = calculate_full_road_length(way_row, df_filtered_nodes)\n",
    "                    if length is not None:\n",
    "                        road_lengths.append({'id': way_row['id'], 'length': length})\n",
    "\n",
    "                df_road_length = pd.DataFrame(road_lengths)\n",
    "                ways = ways.merge(df_road_length, on='id', how='left')\n",
    "\n",
    "                df_filtered = pd.concat([df_filtered_nodes, ways])\n",
    "                df_filtered = df_filtered.fillna('')\n",
    "                print(\"Processing Intersection filter...\")\n",
    "                df_filtered = filter_intersection_nodes(df_filtered)\n",
    "\n",
    "                print(\"Processing 2 degree removal...\")\n",
    "                df_filtered = remove_degree_2_nodes_from_file(df_filtered, ways_filter)\n",
    "                df_filtered = df_filtered.fillna('')\n",
    "            \n",
    "                save_to_xml(df_filtered, city, addresstype)\n",
    "            \n",
    "            #print(\"Processing merging...\")\n",
    "            #df_filtered = process_merging(df_filtered)\n",
    "\n",
    "            print(\"Processing Isolation filter...\")\n",
    "            df_filtered = remove_isolated_nodes(df_filtered)\n",
    "\n",
    "            save_to_xml(df_filtered, city, addresstype)\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Stadt {city}: {e}\")\n",
    "\n",
    "process_cities(cities, ways_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finale Graphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data = load_from_xml_with_length(city_name, addresstype)\n",
    "node_degrees = dict(G.degree()) \n",
    "\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_text = []  \n",
    "node_color = [] \n",
    "\n",
    "for index, row in city_data.iterrows():\n",
    "    if pd.notnull(row['longitude']) and pd.notnull(row['latitude']):\n",
    "        node_x.append(row['longitude'])\n",
    "        node_y.append(row['latitude'])\n",
    "        \n",
    "        hover_info = f\"ID: {row['id']}<br>Tag Key: {row['tagkey']}<br>Tag Value: {row['tagvalue']}<br>Degree: {node_degrees[row['id']]}\"\n",
    "        node_text.append(hover_info)\n",
    "        \n",
    "        node_color.append(node_degrees[row['id']])\n",
    "\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "dashed_edge_x = []\n",
    "dashed_edge_y = []\n",
    "edge_street_names = []\n",
    "\n",
    "for edge in G.edges():\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "\n",
    "    # Überprüfen, ob der Rand highway != 'residual' ist\n",
    "    is_dashed_highway = False\n",
    "    street_name = \"\"\n",
    "    for street in city_data[city_data['type'] == 'way'].itertuples():\n",
    "        if edge[0] in street.node_ids and edge[1] in street.node_ids:\n",
    "            street_name = street.tagvalue\n",
    "            # Highway-Kanten, die nicht 'residual' sind, werden gestrichelt\n",
    "            if street.tagkey == \"highway\" and street.tagvalue not in ['residential', 'unclassified', 'service', 'tertiary', 'secondary']:\n",
    "                is_dashed_highway = True\n",
    "            break\n",
    "\n",
    "    if is_dashed_highway:\n",
    "        dashed_edge_x.extend([x0, x1, None])\n",
    "        dashed_edge_y.extend([y0, y1, None])\n",
    "    else:\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    # Berechnung der Mittelpunkte für die Straßennamen\n",
    "    midpoint_x = (x0 + x1) / 2\n",
    "    midpoint_y = (y0 + y1) / 2\n",
    "    edge_street_names.append((midpoint_x, midpoint_y, street_name))\n",
    "\n",
    "\n",
    "# Plot für durchgezogene Kanten\n",
    "solid_edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines'\n",
    ")\n",
    "\n",
    "# Plot für gestrichelte Kanten (highways)\n",
    "dashed_edge_trace = go.Scatter(\n",
    "    x=dashed_edge_x, y=dashed_edge_y,\n",
    "    line=dict(width=0.5, color='#888', dash='dash'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines'\n",
    ")\n",
    "\n",
    "# Plot für Knoten\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale='YlGnBu',\n",
    "        color=node_color,\n",
    "        size=10,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            title='Knoten Verbindungen',\n",
    "            xanchor='left',\n",
    "            titleside='right'\n",
    "        )),\n",
    "    text=node_text\n",
    ")\n",
    "\n",
    "# Hinzufügen der Straßennamen zwischen den Knoten\n",
    "street_name_trace = go.Scatter(\n",
    "    x=[name[0] for name in edge_street_names],\n",
    "    y=[name[1] for name in edge_street_names],\n",
    "    text=[name[2] for name in edge_street_names],\n",
    "    mode='text',\n",
    "    textfont=dict(\n",
    "        size=12,\n",
    "        color='darkblue'\n",
    "    ),\n",
    "    hoverinfo='text'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=city_name,\n",
    "    titlefont=dict(size=16),\n",
    "    showlegend=False,\n",
    "    hovermode='closest',\n",
    "    margin=dict(b=0, l=0, r=0, t=0),\n",
    "    annotations=[dict(\n",
    "        text=f\"{city_name}\",\n",
    "        showarrow=False,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.005, y=-0.002)],\n",
    "    xaxis=dict(showgrid=False, zeroline=False, visible=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, visible=False)\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[solid_edge_trace, dashed_edge_trace, node_trace], layout=layout)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auswertungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straßennetzwerke und ihre Muster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Straßennetzwerke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samu2\\AppData\\Local\\Temp\\ipykernel_30664\\420716798.py:37: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colors = plt.cm.get_cmap('tab20', len(highway_types))\n"
     ]
    }
   ],
   "source": [
    "city_folder = \"City_data\"\n",
    "files = []\n",
    "for subfolder in ['village', 'town', 'city']:\n",
    "    full_path = os.path.join(city_folder, subfolder)\n",
    "    files.extend([os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")])\n",
    "\n",
    "output_folder = \"Street Network\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for file in files:\n",
    "    city_name = os.path.splitext(os.path.basename(file))[0]\n",
    "    df_city_data = load_from_all_xml(file)\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Knoten hinzufügen\n",
    "    for index, row in df_city_data[df_city_data['type'] == 'node'].iterrows():\n",
    "        G.add_node(row['id'], pos=(row['longitude'], row['latitude']))\n",
    "\n",
    "    # Kanten hinzufügen\n",
    "    for index, way in df_city_data[df_city_data['type'] == 'way'].iterrows():\n",
    "        node_ids_list = way['node_ids']\n",
    "        if node_ids_list is not None:\n",
    "            filtered_node_ids_list = [node_id for node_id in node_ids_list if G.has_node(node_id)]\n",
    "            if len(filtered_node_ids_list) > 1:\n",
    "                highway_type = way['tagvalue']\n",
    "                for i in range(len(filtered_node_ids_list) - 1):\n",
    "                    G.add_edge(\n",
    "                        filtered_node_ids_list[i], filtered_node_ids_list[i + 1],\n",
    "                        street_name=highway_type,\n",
    "                        highway_type=highway_type\n",
    "                    )\n",
    "\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "    highway_types = set(d['highway_type'] for u, v, d in G.edges(data=True))\n",
    "    colors = plt.cm.get_cmap('tab20', len(highway_types))\n",
    "    highway_color_map = {highway_type: colors(i) for i, highway_type in enumerate(highway_types)}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 16))\n",
    "\n",
    "    for highway_type, color in highway_color_map.items():\n",
    "        edges_by_type = [(u, v) for u, v, d in G.edges(data=True) if d.get('highway_type') == highway_type]\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=edges_by_type, edge_color=[color]*len(edges_by_type), width=1.5, ax=ax)\n",
    "\n",
    "    node_degrees = dict(G.degree())\n",
    "    node_color = ['red' if node_degrees[node] >= 3 else 'blue' for node in G.nodes()]\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=1, node_color=node_color, ax=ax)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color=highway_color_map[highway_type], lw=2, label=highway_type)\n",
    "                       for highway_type in highway_color_map]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    ax.set_aspect(1.0 / np.cos(50 * np.pi / 180))\n",
    "\n",
    "    # For removing extra space\n",
    "    #x_values, y_values = zip(*pos.values())\n",
    "    #ax.set_xlim(min(x_values), max(x_values))\n",
    "    #ax.set_ylim(min(y_values), max(y_values))\n",
    "    #ax.set_axis_off()\n",
    "\n",
    "    # Netzwerk speichern\n",
    "    output_file = os.path.join(output_folder, f\"{city_name}.png\")\n",
    "    plt.savefig(output_file, bbox_inches='tight', pad_inches=0, dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straßendiversität"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verteilung der Straßentypen anhand der Länge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_highway_types():\n",
    "    city_folder = \"City_data\"\n",
    "    highway_lengths = defaultdict(float)\n",
    "\n",
    "    for root, dirs, files in os.walk(city_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".osm\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = load_from_all_xml(file_path)\n",
    "\n",
    "                highway_df = df[(df['type'] == 'way') & (df['tagkey'] == 'highway') & (df['length'].notna())]\n",
    "                \n",
    "                # Without track\n",
    "                #highway_df = highway_df[highway_df['tagvalue'] != 'track']\n",
    "\n",
    "                length_sums = highway_df.groupby('tagvalue')['length'].sum()\n",
    "\n",
    "                for highway_type, total_length in length_sums.items():\n",
    "                    highway_lengths[highway_type] += total_length\n",
    "\n",
    "    highway_lengths_df = pd.DataFrame.from_dict(highway_lengths, orient='index', columns=['total_length']).reset_index()\n",
    "    highway_lengths_df.rename(columns={'index': 'highway_type'}, inplace=True)\n",
    "    highway_lengths_df.sort_values(by='total_length', ascending=False, inplace=True)\n",
    "\n",
    "    return highway_lengths_df\n",
    "\n",
    "highway_lengths_df = sum_highway_types()\n",
    "output_folder = \"Street diversity\"\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='total_length', y='highway_type', data=highway_lengths_df)\n",
    "plt.title('Gesamtstraßenlänge pro Straßenart in allen Dateien')\n",
    "plt.xlabel('Gesamtstraßenlänge (m)')\n",
    "plt.ylabel('Straßenart')\n",
    "plt.savefig(os.path.join(output_folder, \"Total_street_length_of_each_type.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verteilung der Straßentypen anhand der Länge pro Stadt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage_highway_lengths_with_limit():\n",
    "    city_folder = \"City_data\"\n",
    "    result = []\n",
    "\n",
    "    for subfolder in ['village', 'town', 'city']:\n",
    "        full_path = os.path.join(city_folder, subfolder)\n",
    "        files = [os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")]\n",
    "\n",
    "        for file_path in files:\n",
    "            df = load_from_all_xml(file_path)\n",
    "\n",
    "            highway_df = df[(df['type'] == 'way') & (df['tagkey'] == 'highway')]\n",
    "\n",
    "            # Without track\n",
    "            #highway_df = highway_df[highway_df['tagvalue'] != 'track']\n",
    "\n",
    "            total_length = highway_df['length'].sum()\n",
    "            \n",
    "            if total_length > 0:\n",
    "                length_by_type = highway_df.groupby('tagvalue')['length'].sum()\n",
    "                length_by_type_sorted = length_by_type.sort_values(ascending=False).head(7)\n",
    "                for highway_type, length in length_by_type_sorted.items():\n",
    "                    percentage = (length / total_length) * 100\n",
    "                    result.append({\n",
    "                        'city': os.path.basename(file_path).split('.')[0],\n",
    "                        'highway_type': highway_type,\n",
    "                        'percentage': percentage\n",
    "                    })\n",
    "\n",
    "    percentage_lengths_df = pd.DataFrame(result)\n",
    "    return percentage_lengths_df\n",
    "\n",
    "percentage_lengths_df = calculate_percentage_highway_lengths_with_limit()\n",
    "output_folder = \"Street diversity\"\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    x='city', y='percentage', hue='highway_type',\n",
    "    data=percentage_lengths_df, palette='muted'\n",
    ")\n",
    "plt.title('Prozentualer Anteil der Straßentypen zur Gesamtlänge in jeder Stadt')\n",
    "plt.xlabel('Stadt')\n",
    "plt.ylabel('Prozentualer Anteil der Gesamtlänge (%)')\n",
    "plt.legend(title='Straßentyp')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_folder, \"Street_length_of_type_to_respective_city.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage_highway_lengths():\n",
    "    city_folder = \"City_data\"\n",
    "    result = []\n",
    "\n",
    "    for subfolder in ['village', 'town', 'city']:\n",
    "        full_path = os.path.join(city_folder, subfolder)\n",
    "        files = [os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")]\n",
    "\n",
    "        for file_path in files:\n",
    "            df = load_from_all_xml(file_path)\n",
    "\n",
    "            highway_df = df[(df['type'] == 'way') & (df['tagkey'] == 'highway')]\n",
    "\n",
    "            # Without track\n",
    "            #highway_df = highway_df[highway_df['tagvalue'] != 'track']\n",
    "\n",
    "            total_length = highway_df['length'].sum()\n",
    "            \n",
    "            if total_length > 0:\n",
    "                length_by_type = highway_df.groupby('tagvalue')['length'].sum()\n",
    "                length_by_type_sorted = length_by_type.sort_values(ascending=False)\n",
    "                for highway_type, length in length_by_type_sorted.items():\n",
    "                    percentage = (length / total_length) * 100\n",
    "                    result.append({\n",
    "                        'city': os.path.basename(file_path).split('.')[0],\n",
    "                        'highway_type': highway_type,\n",
    "                        'percentage': percentage\n",
    "                    })\n",
    "\n",
    "    percentage_lengths_df = pd.DataFrame(result)\n",
    "    return percentage_lengths_df\n",
    "\n",
    "percentage_lengths_df = calculate_percentage_highway_lengths()\n",
    "output_folder = \"Street diversity\"\n",
    "\n",
    "for city in percentage_lengths_df['city'].unique():\n",
    "    city_df = percentage_lengths_df[percentage_lengths_df['city'] == city]\n",
    "\n",
    "    city_output_folder = os.path.join(output_folder, city)\n",
    "    os.makedirs(city_output_folder, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='highway_type', y='percentage', data=city_df, palette='muted')\n",
    "    plt.title(f'Prozentualer Anteil der Straßentypen in {city}')\n",
    "    plt.xlabel('Straßentyp')\n",
    "    plt.ylabel('Prozentualer Anteil der Gesamtlänge (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(city_output_folder, f\"Street_length_percentage_{city}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon-Wiener-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_wiener_index(probabilities):\n",
    "    return -np.sum(probabilities * np.log(probabilities))\n",
    "\n",
    "def calculate_shannon_wiener_index():\n",
    "    city_folder = \"City_data\"\n",
    "    result = []\n",
    "\n",
    "    for subfolder in ['village', 'town', 'city']:\n",
    "        full_path = os.path.join(city_folder, subfolder)\n",
    "        files = [os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")]\n",
    "\n",
    "        for file_path in files:\n",
    "            df = load_from_all_xml(file_path)\n",
    "\n",
    "            highway_df = df[(df['type'] == 'way') & (df['tagkey'] == 'highway')]\n",
    "\n",
    "            total_length = highway_df['length'].sum()\n",
    "\n",
    "            if total_length > 0:\n",
    "                length_by_type = highway_df.groupby('tagvalue')['length'].sum()\n",
    "                length_by_type_sorted = length_by_type / total_length  \n",
    "                sw_index = shannon_wiener_index(length_by_type_sorted)\n",
    "                result.append({\n",
    "                    'city': os.path.basename(file_path).split('.')[0],\n",
    "                    'shannon_wiener_index': sw_index\n",
    "                })\n",
    "\n",
    "    sw_index_df = pd.DataFrame(result)\n",
    "    return sw_index_df\n",
    "\n",
    "sw_index_df = calculate_shannon_wiener_index()\n",
    "output_folder = \"Street diversity\"\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='city', y='shannon_wiener_index', data=sw_index_df, palette='viridis')\n",
    "plt.title('Shannon-Wiener-Index der Straßenvielfalt in verschiedenen Städten')\n",
    "plt.xlabel('Stadt')\n",
    "plt.ylabel('Shannon-Wiener-Index')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_folder, \"Shannon_Wiener_Index_Cities.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dijkstra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, city):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for index, row in df[df['type'] == 'node'].iterrows():  \n",
    "        G.add_node(row['id'], pos=(row['longitude'], row['latitude']))\n",
    "\n",
    "    for index, way in df[df['type'] == 'way'].iterrows():\n",
    "        highway_type = way['tagvalue']\n",
    "        if highway_type == 'track':\n",
    "            continue\n",
    "        \n",
    "        node_ids_list = way['node_ids']\n",
    "        if node_ids_list is not None:\n",
    "            filtered_node_ids_list = [node_id for node_id in node_ids_list if G.has_node(node_id)]\n",
    "            if len(filtered_node_ids_list) > 1:\n",
    "                for i in range(len(filtered_node_ids_list) - 1):\n",
    "                    G.add_edge(filtered_node_ids_list[i], filtered_node_ids_list[i + 1], highway_type=highway_type)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def select_random_node_pairs(df_nodes, num_pairs=10, min_distance=1000):\n",
    "    random_pairs = []\n",
    "\n",
    "    for _ in range(num_pairs):\n",
    "        start_node = random.choice(df_nodes['id'].values)\n",
    "        end_node = random.choice(df_nodes['id'].values)\n",
    "\n",
    "        while start_node == end_node or haversine(\n",
    "            df_nodes[df_nodes['id'] == start_node]['longitude'].values[0], \n",
    "            df_nodes[df_nodes['id'] == start_node]['latitude'].values[0],\n",
    "            df_nodes[df_nodes['id'] == end_node]['longitude'].values[0],\n",
    "            df_nodes[df_nodes['id'] == end_node]['latitude'].values[0]\n",
    "        ) < min_distance:\n",
    "            start_node = random.choice(df_nodes['id'].values)\n",
    "            end_node = random.choice(df_nodes['id'].values)\n",
    "\n",
    "        random_pairs.append((start_node, end_node))\n",
    "\n",
    "    return random_pairs\n",
    "\n",
    "def run_dijkstra_and_track_types(G, start_node, end_node):\n",
    "    try:\n",
    "        shortest_path = nx.dijkstra_path(G, source=start_node, target=end_node, weight='weight')\n",
    "    except nx.NetworkXNoPath:\n",
    "        print(f\"No path found between {start_node} and {end_node}, selecting new nodes.\")\n",
    "        return None\n",
    "    \n",
    "    types_traversed = []\n",
    "    for i in range(len(shortest_path) - 1):\n",
    "        node1 = shortest_path[i]\n",
    "        node2 = shortest_path[i + 1]\n",
    "        edge_data = G.get_edge_data(node1, node2)\n",
    "        highway_type = edge_data['highway_type']\n",
    "        types_traversed.append(highway_type)\n",
    "\n",
    "    return types_traversed\n",
    "\n",
    "def plot_traffic_efficiency(results_df, output_folder, analysis_number):\n",
    "    dijkstra_folder = os.path.join(output_folder, 'Dijkstra')\n",
    "    os.makedirs(dijkstra_folder, exist_ok=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='highway_type', y='count', hue='city', data=results_df, palette='muted')\n",
    "    plt.title(f'Verkehrseffizienz: Häufigkeit der durchfahrenen Straßentypen (Analyse {analysis_number})')\n",
    "    plt.xlabel('Straßentyp')\n",
    "    plt.ylabel('Durchfahrten (Anzahl)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(dijkstra_folder, f\"Dijkstra_Analysis_{analysis_number}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def perform_dijkstra_analysis():\n",
    "    city_folder = \"City_data\"\n",
    "    files = []\n",
    "    output_folder = \"Street diversity\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for subfolder in ['village', 'town', 'city']:\n",
    "        full_path = os.path.join(city_folder, subfolder)\n",
    "        files.extend([os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")])\n",
    "\n",
    "    for analysis_number in range(1, 11):\n",
    "        results = []\n",
    "        for file in files:\n",
    "            city_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            df = load_from_all_xml(file)  \n",
    "            G = create_graph(df, city_name)  \n",
    "\n",
    "            df_nodes = df[df['type'] == 'node']\n",
    "            node_pairs = select_random_node_pairs(df_nodes)\n",
    "\n",
    "            all_types = []\n",
    "            for start_node, end_node in node_pairs:\n",
    "                types_traversed = run_dijkstra_and_track_types(G, start_node, end_node)\n",
    "                while types_traversed is None:\n",
    "                    start_node, end_node = random.choice(node_pairs)\n",
    "                    types_traversed = run_dijkstra_and_track_types(G, start_node, end_node)\n",
    "                all_types.extend(types_traversed)\n",
    "\n",
    "            type_counts = pd.Series(all_types).value_counts()\n",
    "            for highway_type, count in type_counts.items():\n",
    "                results.append({'city': city_name, 'highway_type': highway_type, 'count': count})\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        plot_traffic_efficiency(results_df, output_folder, analysis_number)\n",
    "\n",
    "perform_dijkstra_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straßenlängen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplott der Straßenlängen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def street_length_comparison():\n",
    "    city_folder = \"City_data\"\n",
    "    files = []\n",
    "    for subfolder in ['village', 'town', 'city']:\n",
    "        full_path = os.path.join(city_folder, subfolder)\n",
    "        files.extend([os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")])\n",
    "\n",
    "    all_lengths = []\n",
    "\n",
    "    for file in files:\n",
    "        df = load_from_all_xml(file)\n",
    "\n",
    "        # Nur Straßen mit Längen auswählen\n",
    "        df_filtered = df[(df['type'] == 'way') & df['length'].notna()]\n",
    "        city_name = os.path.basename(file).replace(\".osm\", \"\")\n",
    "        df_filtered['city'] = city_name\n",
    "        all_lengths.append(df_filtered[['length', 'city']])\n",
    "\n",
    "    df_all_lengths = pd.concat(all_lengths, ignore_index=True)\n",
    "\n",
    "    output_folder = \"Street length of various places\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.boxplot(data=df_all_lengths, x='city', y='length')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\"Straßenlängen pro Stadt\")\n",
    "    plt.xlabel(\"Stadt\")\n",
    "    plt.ylabel(\"Länge (m)\")\n",
    "    plt.ylim(0, 2000)\n",
    "    plt.savefig(os.path.join(output_folder, \"Street_length_comparison.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramm zu den Straßenlängen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def street_length_per_city():\n",
    "    city_folder = \"City_data\"\n",
    "    files = []\n",
    "    for subfolder in ['village', 'town', 'city']:\n",
    "        full_path = os.path.join(city_folder, subfolder)\n",
    "        files.extend([os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")])\n",
    "\n",
    "    for file in files:\n",
    "        city_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        df = load_from_all_xml(file)\n",
    "\n",
    "        df_filtered = df[(df['type'] == 'way') & df['length'].notna()]\n",
    "\n",
    "        output_folder = \"Street length of various places\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        city_output_folder = os.path.join(output_folder, city_name)\n",
    "        os.makedirs(city_output_folder, exist_ok=True)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df_filtered['length'], bins=30, kde=True)\n",
    "        plt.xscale(\"log\")\n",
    "        plt.title(f\"Straßenlängen in {city_name}\")\n",
    "        plt.xlabel(\"Straßenlänge (log)\")\n",
    "        plt.ylabel(\"Anzahl der Straßen\")\n",
    "        plt.savefig(os.path.join(city_output_folder, f\"Street_length_{city_name}.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramm zu den Bundesstraßen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federal_highways_per_city():\n",
    "    city_folder = \"City_data\"\n",
    "    highway_types = ['primary', 'secondary', 'tertiary']\n",
    "    colors = {'primary': 'skyblue', 'secondary': 'orange', 'tertiary': 'green'}\n",
    "    files = []\n",
    "    for subfolder in ['village', 'town', 'city']:\n",
    "        full_path = os.path.join(city_folder, subfolder)\n",
    "        files.extend([os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")])\n",
    "\n",
    "    for file in files:\n",
    "        city_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        df = load_from_all_xml(file)\n",
    "\n",
    "        output_folder = \"Street length of various places\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "        city_output_folder = os.path.join(output_folder, city_name)\n",
    "        os.makedirs(city_output_folder, exist_ok=True)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for highway in highway_types:\n",
    "            df_filtered = df[(df['type'] == 'way') & \n",
    "                             (df['tagkey'] == 'highway') & \n",
    "                             (df['tagvalue'] == highway) & \n",
    "                             df['length'].notna()]\n",
    "\n",
    "            if not df_filtered.empty:\n",
    "                sns.histplot(\n",
    "                    df_filtered['length'], bins=30, kde=True, \n",
    "                    color=colors[highway], label=highway\n",
    "                )\n",
    "\n",
    "        plt.xscale(\"log\")\n",
    "        plt.title(f\"Straßenlängen in {city_name}\")\n",
    "        plt.xlabel(\"Straßenlänge (log)\")\n",
    "        plt.ylabel(\"Anzahl der Straßen\")\n",
    "        plt.legend(title=\"Highway-Typ\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(city_output_folder, f\"Federal_highways_length_{city_name}.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramm zu Stadtstraßen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_streets_per_city():\n",
    "    city_folder = \"City_data\"\n",
    "    highway_types = ['residential', 'living_street']\n",
    "    colors = {'residential': 'skyblue', 'living_street': 'orange'}\n",
    "    files = []\n",
    "    for subfolder in ['village', 'town', 'city']:\n",
    "        full_path = os.path.join(city_folder, subfolder)\n",
    "        files.extend([os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")])\n",
    "\n",
    "    for file in files:\n",
    "        city_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        df = load_from_all_xml(file)\n",
    "\n",
    "        output_folder = \"Street length of various places\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "        city_output_folder = os.path.join(output_folder, city_name)\n",
    "        os.makedirs(city_output_folder, exist_ok=True)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for highway in highway_types:\n",
    "            df_filtered = df[(df['type'] == 'way') & \n",
    "                             (df['tagkey'] == 'highway') & \n",
    "                             (df['tagvalue'] == highway) & \n",
    "                             df['length'].notna()]\n",
    "\n",
    "            if not df_filtered.empty:\n",
    "                sns.histplot(\n",
    "                    df_filtered['length'], bins=30, kde=True, \n",
    "                    color=colors[highway], label=highway\n",
    "                )\n",
    "\n",
    "        plt.xscale(\"log\")\n",
    "        plt.title(f\"Straßenlängen in {city_name}\")\n",
    "        plt.xlabel(\"Straßenlänge (log)\")\n",
    "        plt.ylabel(\"Anzahl der Straßen\")\n",
    "        plt.legend(title=\"Highway-Typ\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(city_output_folder, f\"City_street_length_{city_name}.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramm für Fußwege"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def footpaths_per_city():\n",
    "    city_folder = \"City_data\"\n",
    "    highway_types = ['service', 'pedestrian', 'track', 'footway', 'sidewalk', 'steps']\n",
    "    colors = {'service': 'skyblue', 'pedestrian': 'orange', 'track': 'green', 'footway': 'yellow', 'sidewalk': 'red', 'steps': 'purple'}\n",
    "    files = []\n",
    "    for subfolder in ['village', 'town', 'city']:\n",
    "        full_path = os.path.join(city_folder, subfolder)\n",
    "        files.extend([os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")])\n",
    "\n",
    "    for file in files:\n",
    "        city_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        df = load_from_all_xml(file)\n",
    "\n",
    "        output_folder = \"Street length of various places\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "        city_output_folder = os.path.join(output_folder, city_name)\n",
    "        os.makedirs(city_output_folder, exist_ok=True)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for highway in highway_types:\n",
    "            df_filtered = df[(df['type'] == 'way') & \n",
    "                             (df['tagkey'] == 'highway') & \n",
    "                             (df['tagvalue'] == highway) & \n",
    "                             df['length'].notna()]\n",
    "\n",
    "            if not df_filtered.empty:\n",
    "                sns.histplot(\n",
    "                    df_filtered['length'], bins=30, kde=True, \n",
    "                    color=colors[highway], label=highway\n",
    "                )\n",
    "\n",
    "        plt.xscale(\"log\")\n",
    "        plt.title(f\"Straßenlängen in {city_name}\")\n",
    "        plt.xlabel(\"Straßenlänge (log)\")\n",
    "        plt.ylabel(\"Anzahl der Straßen\")\n",
    "        plt.legend(title=\"Highway-Typ\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(city_output_folder, f\"Footpaths_length_{city_name}.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_length_comparison()\n",
    "street_length_per_city()\n",
    "federal_highways_per_city()\n",
    "city_streets_per_city()\n",
    "footpaths_per_city()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netzwerkdichte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramm für Knotengrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_folder = \"City_data\"\n",
    "files = []\n",
    "for subfolder in ['village', 'town', 'city']:\n",
    "    full_path = os.path.join(city_folder, subfolder)\n",
    "    files.extend([os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")])\n",
    "\n",
    "columns = 3\n",
    "rows = (len(files) + columns - 1) // columns  \n",
    "fig, axes = plt.subplots(rows, columns, figsize=(15, 5 * rows), squeeze=False)\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    city_name = os.path.splitext(os.path.basename(file))[0]\n",
    "    city_data = load_from_all_xml(file)\n",
    "\n",
    "    # Netzwerkgraph für diese Stadt erstellen\n",
    "    G = nx.Graph()\n",
    "    for _, row in city_data[city_data['type'] == 'way'].iterrows():\n",
    "        node_ids = row['node_ids']\n",
    "        if node_ids is not None:\n",
    "            for i in range(len(node_ids) - 1):\n",
    "                G.add_edge(node_ids[i], node_ids[i + 1])\n",
    "\n",
    "    # Knotengrade berechnen\n",
    "    node_degrees = dict(G.degree())\n",
    "    city_data['degree'] = city_data['id'].map(node_degrees).fillna(0).astype(int)\n",
    "    city_data = city_data[city_data['degree'] > 0]  # Knoten ohne Verbindungen entfernen\n",
    "\n",
    "    # Knotengrade auf maximal 6 beschränken\n",
    "    city_data = city_data[city_data['degree'] <= 6]\n",
    "    city_node_degrees = city_data['degree']\n",
    "\n",
    "    # Position im Gitterlayout berechnen\n",
    "    row, col = divmod(idx, columns)\n",
    "    ax = axes[row][col]\n",
    "\n",
    "    bins = np.arange(1, 7) - 0.5\n",
    "    \n",
    "    sns.histplot(city_node_degrees, bins=bins, kde=True, ax=ax)  \n",
    "    ax.set_title(f\"Knotengrade in {city_name}\")\n",
    "    ax.set_xlabel(\"Knotengrad\")\n",
    "    ax.set_ylabel(\"Anzahl der Knoten\")\n",
    "    ax.set_xticks(range(1, 7))\n",
    "\n",
    "for idx in range(len(files), rows * columns):\n",
    "    row, col = divmod(idx, columns)\n",
    "    axes[row][col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-basierte Analyse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grid_density(df_nodes, df_ways, grid_size=200):\n",
    "    earth_radius = 6371000\n",
    "    df_nodes['x'] = np.radians(df_nodes['longitude']) * np.cos(np.radians(df_nodes['latitude'])) * earth_radius\n",
    "    df_nodes['y'] = np.radians(df_nodes['latitude']) * earth_radius\n",
    "\n",
    "    df_nodes['grid_x'] = (df_nodes['x'] // grid_size).astype(int)\n",
    "    df_nodes['grid_y'] = (df_nodes['y'] // grid_size).astype(int)\n",
    "\n",
    "    edges = []\n",
    "    for _, row in df_ways.iterrows():\n",
    "        node_ids = row['node_ids']\n",
    "        if node_ids is not None:\n",
    "            for i in range(len(node_ids) - 1):\n",
    "                edges.append((node_ids[i], node_ids[i + 1]))\n",
    "\n",
    "    edges_df = pd.DataFrame(edges, columns=['node1', 'node2'])\n",
    "    edges_df = edges_df.merge(df_nodes[['id', 'grid_x', 'grid_y']], left_on='node1', right_on='id', how='left').rename(columns={'grid_x': 'grid_x1', 'grid_y': 'grid_y1'}).drop(columns=['id'])\n",
    "    edges_df = edges_df.merge(df_nodes[['id', 'grid_x', 'grid_y']], left_on='node2', right_on='id', how='left').rename(columns={'grid_x': 'grid_x2', 'grid_y': 'grid_y2'}).drop(columns=['id'])\n",
    "\n",
    "    edges_df = edges_df[(edges_df['grid_x1'] == edges_df['grid_x2']) & (edges_df['grid_y1'] == edges_df['grid_y2'])]\n",
    "\n",
    "    edges_df['grid_x'] = edges_df['grid_x1']\n",
    "    edges_df['grid_y'] = edges_df['grid_y1']\n",
    "\n",
    "    edge_density = edges_df.groupby(['grid_x', 'grid_y']).size().reset_index(name='edge_count')\n",
    "    return edge_density\n",
    "\n",
    "\n",
    "city_folder = \"City_data\"\n",
    "files = []\n",
    "for subfolder in ['village', 'town', 'city']:\n",
    "    full_path = os.path.join(city_folder, subfolder)\n",
    "    files.extend([os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")])\n",
    "all_densities = []\n",
    "\n",
    "for file in files:\n",
    "    city_name = os.path.splitext(os.path.basename(file))[0]\n",
    "    df = load_from_all_xml(file)\n",
    "    \n",
    "    df_nodes = df[df['type'] == 'node']\n",
    "    df_ways = df[df['type'] == 'way']\n",
    "    \n",
    "    # Berechne Knotendichte\n",
    "    grid_density = calculate_grid_density(df_nodes, df_ways)\n",
    "    grid_density['city_name'] = city_name\n",
    "    all_densities.append(grid_density)\n",
    "\n",
    "output_folder = \"Density of various places\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Kombiniere die Knotendichten aller Städte\n",
    "df_all_densities = pd.concat(all_densities, ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df_all_densities, x='city_name', y='edge_count')\n",
    "plt.title(\"Vergleich der Netzwerkdichte im 200m-Gitternetz\")\n",
    "plt.xlabel(\"Stadt\")\n",
    "plt.ylim(0, 25)\n",
    "plt.ylabel(\"Netzwerkdichte (Anzahl Kanten pro Gitterzelle)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_folder, \"Network_density_comparison_200m_Grid.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grid_density(df_nodes, df_ways, grid_size=200):\n",
    "    earth_radius = 6371000\n",
    "    df_nodes['x'] = np.radians(df_nodes['longitude']) * np.cos(np.radians(df_nodes['latitude'])) * earth_radius\n",
    "    df_nodes['y'] = np.radians(df_nodes['latitude']) * earth_radius\n",
    "\n",
    "    df_nodes['grid_x'] = (df_nodes['x'] // grid_size).astype(int)\n",
    "    df_nodes['grid_y'] = (df_nodes['y'] // grid_size).astype(int)\n",
    "\n",
    "    edges = []\n",
    "    for _, row in df_ways.iterrows():\n",
    "        node_ids = row['node_ids']\n",
    "        if node_ids is not None:\n",
    "            for i in range(len(node_ids) - 1):\n",
    "                edges.append((node_ids[i], node_ids[i + 1]))\n",
    "\n",
    "    edges_df = pd.DataFrame(edges, columns=['node1', 'node2'])\n",
    "    edges_df = edges_df.merge(df_nodes[['id', 'grid_x', 'grid_y']], left_on='node1', right_on='id', how='left').rename(columns={'grid_x': 'grid_x1', 'grid_y': 'grid_y1'}).drop(columns=['id'])\n",
    "    edges_df = edges_df.merge(df_nodes[['id', 'grid_x', 'grid_y']], left_on='node2', right_on='id', how='left').rename(columns={'grid_x': 'grid_x2', 'grid_y': 'grid_y2'}).drop(columns=['id'])\n",
    "\n",
    "    edges_df = edges_df[(edges_df['grid_x1'] == edges_df['grid_x2']) & (edges_df['grid_y1'] == edges_df['grid_y2'])]\n",
    "\n",
    "    edges_df['grid_x'] = edges_df['grid_x1']\n",
    "    edges_df['grid_y'] = edges_df['grid_y1']\n",
    "\n",
    "    edge_density = edges_df.groupby(['grid_x', 'grid_y']).size().reset_index(name='edge_count')\n",
    "    return edge_density\n",
    "\n",
    "city_folder = \"City_data\"\n",
    "output_folder = \"Density of various places\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for subfolder in ['village', 'town', 'city']:\n",
    "    full_path = os.path.join(city_folder, subfolder)\n",
    "    files = [os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")]\n",
    "\n",
    "    for file in files:\n",
    "        city_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        df = load_from_all_xml(file)\n",
    "\n",
    "        df_nodes = df[df['type'] == 'node']\n",
    "        df_ways = df[df['type'] == 'way']\n",
    "\n",
    "        grid_density = calculate_grid_density(df_nodes, df_ways)\n",
    "\n",
    "        city_output_folder = os.path.join(output_folder, city_name)\n",
    "        os.makedirs(city_output_folder, exist_ok=True)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(\n",
    "            data=grid_density,\n",
    "            x='grid_x', y='grid_y',\n",
    "            size='edge_count', sizes=(10, 200),\n",
    "            hue='edge_count', palette='viridis',\n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.title(f\"Netzwerkdichte im 200m-Gitternetz von {city_name}\")\n",
    "        plt.xlabel(\"Gitter x-Koordinate\")\n",
    "        plt.ylabel(\"Gitter y-Koordinate\")\n",
    "        plt.legend(title='Anzahl Kanten')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(city_output_folder, f\"Network_density_scatter_200m_Grid_{city_name}.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel-basierte Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kernel_density(df_nodes, df_ways):\n",
    "    earth_radius = 6371000\n",
    "    df_nodes['x'] = np.radians(df_nodes['longitude']) * np.cos(np.radians(df_nodes['latitude'])) * earth_radius\n",
    "    df_nodes['y'] = np.radians(df_nodes['latitude']) * earth_radius\n",
    "\n",
    "    edges = []\n",
    "    for _, row in df_ways.iterrows():\n",
    "        node_ids = row['node_ids']\n",
    "        if node_ids is not None:\n",
    "            for i in range(len(node_ids) - 1):\n",
    "                edges.append((node_ids[i], node_ids[i + 1]))\n",
    "\n",
    "    edges_df = pd.DataFrame(edges, columns=['node1', 'node2'])\n",
    "    edges_df = edges_df.merge(df_nodes[['id', 'x', 'y']], left_on='node1', right_on='id', how='left').rename(columns={'x': 'x1', 'y': 'y1'}).drop(columns=['id'])\n",
    "    edges_df = edges_df.merge(df_nodes[['id', 'x', 'y']], left_on='node2', right_on='id', how='left').rename(columns={'x': 'x2', 'y': 'y2'}).drop(columns=['id'])    \n",
    "\n",
    "    edges_df['x'] = (edges_df['x1'] + edges_df['x2']) / 2\n",
    "    edges_df['y'] = (edges_df['y1'] + edges_df['y2']) / 2\n",
    "\n",
    "    return edges_df[['x', 'y']]\n",
    "\n",
    "city_folder = \"City_data\"\n",
    "output_folder = \"Density of various places\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "files = []\n",
    "for subfolder in ['village', 'town', 'city']:\n",
    "    full_path = os.path.join(city_folder, subfolder)\n",
    "    files = [os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")]\n",
    "\n",
    "    for file in files:\n",
    "        city_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        df = load_from_all_xml(file)\n",
    "\n",
    "        df_nodes = df[df['type'] == 'node']\n",
    "        df_ways = df[df['type'] == 'way']\n",
    "\n",
    "        density_points = calculate_kernel_density(df_nodes, df_ways)\n",
    "\n",
    "        city_output_folder = os.path.join(output_folder, city_name)\n",
    "        os.makedirs(city_output_folder, exist_ok=True)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.kdeplot(\n",
    "            data=density_points,\n",
    "            x='x', y='y',\n",
    "            fill=True, common_norm=False, alpha=0.5\n",
    "        )\n",
    "        plt.title(f\"Kernel-Dichteschätzung der Netzwerkdichte in {city_name}\")\n",
    "        plt.xlabel(\"x (in Metern)\")\n",
    "        plt.ylabel(\"y (in Metern)\")\n",
    "        plt.tight_layout()\n",
    "        output_file = os.path.join(city_output_folder, f\"Network_density_kde_{city_name}.png\")\n",
    "        plt.savefig(output_file)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswirkungen von wichtigsten Kreuzungen und Straßen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_for_centrality(df):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Knoten mit Positionsdaten hinzufügen\n",
    "    for index, row in df[df['type'] == 'node'].iterrows():  \n",
    "        G.add_node(row['id'], pos=(row['longitude'], row['latitude']))\n",
    "\n",
    "    # Kanten hinzufügen\n",
    "    for index, way in df[df['type'] == 'way'].iterrows():\n",
    "        node_ids_list = way['node_ids']\n",
    "        if node_ids_list is not None:\n",
    "            filtered_node_ids_list = [node_id for node_id in node_ids_list if G.has_node(node_id)]\n",
    "            if len(filtered_node_ids_list) > 1:\n",
    "                for i in range(len(filtered_node_ids_list) - 1):\n",
    "                    G.add_edge(filtered_node_ids_list[i], filtered_node_ids_list[i + 1])\n",
    "\n",
    "    largest_component = max(nx.connected_components(G), key=len)\n",
    "    G = G.subgraph(largest_component).copy()\n",
    "\n",
    "    return G  \n",
    "\n",
    "def calculate_centrality_indices(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "    return {\n",
    "        'Degree_centrality': degree_centrality,\n",
    "        'Closeness_centrality': closeness_centrality,\n",
    "        'Betweenness_centrality': betweenness_centrality,\n",
    "    }\n",
    "\n",
    "def visualize_centrality(G, centrality, title, output_folder):\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    nodes = nx.draw_networkx_nodes(G, pos, node_size=5, cmap=plt.cm.plasma,\n",
    "                                   node_color=list(centrality.values()), alpha=0.8)\n",
    "    edges = nx.draw_networkx_edges(G, pos, edge_color='grey', alpha=0.5)\n",
    "\n",
    "    # Hinzufügen der Farbskala\n",
    "    cbar = plt.colorbar(nodes)\n",
    "    cbar.set_label('Centrality Value')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(output_folder, f\"{title.replace(' ', '_')}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def get_top_betweennes_centrality_nodes(centrality, top_n):\n",
    "    return sorted(centrality.items(), key=lambda item: item[1], reverse=True)[:top_n]\n",
    "\n",
    "def get_top_closeness_centrality_nodes(centrality, top_n):\n",
    "    return sorted(centrality.items(), key=lambda item: item[1], reverse=True)[:top_n]\n",
    "\n",
    "def calculate_average_shortest_path_length(G):\n",
    "    if not nx.is_connected(G):\n",
    "        G = G.subgraph(max(nx.connected_components(G), key=len)).copy()\n",
    "    return nx.average_shortest_path_length(G)\n",
    "\n",
    "def calculate_number_of_components(G):\n",
    "    return nx.number_connected_components(G)\n",
    "\n",
    "def remove_nodes(G, nodes_to_remove):\n",
    "    G_removed = G.copy()\n",
    "    G_removed.remove_nodes_from(nodes_to_remove)\n",
    "    return G_removed\n",
    "\n",
    "def visualize_centrality_comparison(G_original, G_removed, centrality_original, centrality_removed, title, output_folder):\n",
    "    pos = nx.get_node_attributes(G_original, 'pos')\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f\"Original {title}\")\n",
    "    nx.draw(G_original, pos, node_size=10, cmap=plt.cm.plasma,\n",
    "            node_color=list(centrality_original.values()), alpha=0.8, with_labels=False, edge_color='grey')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f\"After Removal {title}\")\n",
    "    nx.draw(G_removed, pos, node_size=10, cmap=plt.cm.plasma,\n",
    "            node_color=list(centrality_removed.values()), alpha=0.8, with_labels=False, edge_color='grey')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.savefig(os.path.join(output_folder, f\"{title.replace(' ', '_')}_comparison.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def visualize_comparison_metrics(city_names, original_counts, new_counts, metric_name, output_folder, filename):\n",
    "    x = range(len(city_names))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(x, original_counts, width=0.4, label=f'Original {metric_name}', align='center')\n",
    "    plt.bar(x, new_counts, width=0.4, label=f'New {metric_name}', align='edge')\n",
    "    plt.xticks(x, city_names, rotation=45)\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f'Comparison of {metric_name}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, f\"{filename}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darstellung der Zentralitäten pro Stadt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_folder = \"City_data\"\n",
    "output_base_folder = \"Node centrality\"\n",
    "\n",
    "collected_data = []\n",
    "\n",
    "top_n_map = {\n",
    "    'village': 10,\n",
    "    'town': 20,\n",
    "    'city': 30\n",
    "}\n",
    "\n",
    "for subfolder in ['village', 'town']:\n",
    "    full_path = os.path.join(city_folder, subfolder)\n",
    "    files = [os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")]\n",
    "    top_n = top_n_map[subfolder]\n",
    "    for file_path in files:\n",
    "        city_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_folder = os.path.join(output_base_folder, city_name)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        print(f\"Processing {city_name}...\") \n",
    "\n",
    "        df = load_from_all_xml(file_path)\n",
    "        G = create_graph_for_centrality(df)\n",
    "        centrality_indices = calculate_centrality_indices(G)\n",
    "\n",
    "        top_nodes_between = get_top_betweennes_centrality_nodes(centrality_indices['Betweenness_centrality'], top_n=top_n)\n",
    "        top_nodes_close = get_top_closeness_centrality_nodes(centrality_indices['Closeness_centrality'], top_n=top_n)\n",
    "        for node_id, centrality_value in top_nodes_between:\n",
    "            collected_data.append({'city': city_name, 'node_id': node_id, 'Betweenness_centrality': centrality_value})\n",
    "\n",
    "        for node_id, centrality_value in top_nodes_close:\n",
    "            collected_data.append({'city': city_name, 'node_id': node_id, 'Closeness_centrality': centrality_value})\n",
    "\n",
    "        for centrality_name, centrality_values in centrality_indices.items():\n",
    "            visualize_centrality(G, centrality_values, centrality_name.replace(\"_\", \" \").title(), output_folder)\n",
    "\n",
    "top_nodes_df = pd.DataFrame(collected_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich vor und nach Löschen der Top-Knoten (Betweenness und Closeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hauptlogik\n",
    "city_folder = \"City_data\"\n",
    "output_base_folder = \"Node centrality\"\n",
    "\n",
    "city_names = []\n",
    "original_component_counts = []\n",
    "new_component_counts = []\n",
    "original_asp_lengths = []\n",
    "new_asp_lengths = []\n",
    "\n",
    "top_n_map = {\n",
    "    'village': 10,\n",
    "    'town': 20,\n",
    "    'city': 30\n",
    "}\n",
    "\n",
    "for subfolder in ['village', 'town', 'city']:\n",
    "    full_path = os.path.join(city_folder, subfolder)\n",
    "    files = [os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")]\n",
    "    for file_path in files:\n",
    "        city_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_folder = os.path.join(output_base_folder, city_name)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        print(f\"Processing {city_name}...\")    \n",
    "        df = load_from_all_xml(file_path)\n",
    "\n",
    "        G = create_graph_for_centrality(df)\n",
    "        centrality_indices_original = calculate_centrality_indices(G)\n",
    "\n",
    "        original_component_count = calculate_number_of_components(G)\n",
    "        original_asp_length = calculate_average_shortest_path_length(G)\n",
    "\n",
    "        # Top-K Knoten der Betweenness-Zentralität\n",
    "        top_nodes_between = get_top_betweennes_centrality_nodes(centrality_indices_original['Betweenness_centrality'], top_n=top_n_map[subfolder])\n",
    "        nodes_to_remove = [node_id for node_id, _ in top_nodes_between]\n",
    "\n",
    "        G_removed = remove_nodes(G, nodes_to_remove)\n",
    "        centrality_indices_removed = calculate_centrality_indices(G_removed)\n",
    "\n",
    "        new_component_count = calculate_number_of_components(G_removed)\n",
    "        new_asp_length = calculate_average_shortest_path_length(G_removed)\n",
    "\n",
    "        city_names.append(city_name)\n",
    "        original_component_counts.append(original_component_count)\n",
    "        new_component_counts.append(new_component_count)\n",
    "        original_asp_lengths.append(original_asp_length)\n",
    "        new_asp_lengths.append(new_asp_length)\n",
    "\n",
    "        # Visualisierung der zentralen Knoten\n",
    "        visualize_centrality_comparison(\n",
    "            G, G_removed,\n",
    "            centrality_indices_original['Betweenness_centrality'],\n",
    "            centrality_indices_removed['Betweenness_centrality'],\n",
    "            'Betweenness Centrality',\n",
    "            output_folder\n",
    "        )\n",
    "\n",
    "# Vergleichsmetriken visualisieren\n",
    "visualize_comparison_metrics(city_names, original_component_counts, new_component_counts, \n",
    "                             'Number of Components', output_base_folder, \"Number_of_components_Betweenness\")\n",
    "visualize_comparison_metrics(city_names, original_asp_lengths, new_asp_lengths, \n",
    "                             'ASP Lengths', output_base_folder, \"ASP_lengths_Betweenness\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_folder = \"City_data\"\n",
    "output_base_folder = \"Node centrality\"\n",
    "\n",
    "city_names = []\n",
    "original_component_counts = []\n",
    "new_component_counts = []\n",
    "original_asp_lengths = []\n",
    "new_asp_lengths = []\n",
    "\n",
    "top_n_map = {\n",
    "    'village': 10,\n",
    "    'town': 20,\n",
    "    'city': 30\n",
    "}\n",
    "\n",
    "for subfolder in ['village', 'town', 'city']:\n",
    "    full_path = os.path.join(city_folder, subfolder)\n",
    "    files = [os.path.join(full_path, file) for file in os.listdir(full_path) if file.endswith(\".osm\")]\n",
    "    for file_path in files:\n",
    "        city_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_folder = os.path.join(output_base_folder, city_name)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        print(f\"Processing {city_name}...\")    \n",
    "        df = load_from_all_xml(file_path)\n",
    "\n",
    "        G = create_graph_for_centrality(df)\n",
    "        centrality_indices_original = calculate_centrality_indices(G)\n",
    "\n",
    "        original_component_count = calculate_number_of_components(G)\n",
    "        original_asp_length = calculate_average_shortest_path_length(G)\n",
    "\n",
    "        # Top-K Knoten der Closeness-Zentralität\n",
    "        top_nodes_between = get_top_closeness_centrality_nodes(centrality_indices_original['Closeness_centrality'], top_n=top_n_map[subfolder])\n",
    "        nodes_to_remove = [node_id for node_id, _ in top_nodes_between]\n",
    "\n",
    "        G_removed = remove_nodes(G, nodes_to_remove)\n",
    "        centrality_indices_removed = calculate_centrality_indices(G_removed)\n",
    "\n",
    "        new_component_count = calculate_number_of_components(G_removed)\n",
    "        new_asp_length = calculate_average_shortest_path_length(G_removed)\n",
    "\n",
    "        city_names.append(city_name)\n",
    "        original_component_counts.append(original_component_count)\n",
    "        new_component_counts.append(new_component_count)\n",
    "        original_asp_lengths.append(original_asp_length)\n",
    "        new_asp_lengths.append(new_asp_length)\n",
    "\n",
    "        # Visualisierung der zentralen Knoten\n",
    "        visualize_centrality_comparison(\n",
    "            G, G_removed,\n",
    "            centrality_indices_original['Closeness_centrality'],\n",
    "            centrality_indices_removed['Closeness_centrality'],\n",
    "            'Closeness Centrality',\n",
    "            output_folder\n",
    "        )\n",
    "\n",
    "# Vergleichsmetriken visualisieren\n",
    "visualize_comparison_metrics(city_names, original_component_counts, new_component_counts, \n",
    "                             'Number of Components', output_base_folder, \"Number_of_components_Closeness\")\n",
    "visualize_comparison_metrics(city_names, original_asp_lengths, new_asp_lengths, \n",
    "                             'ASP Lengths', output_base_folder, \"ASP_lengths_Closeness\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
